{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/near129/petfinder/blob/master/petfinder/run_exp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g0hLq3k3grLU"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# @„ÅØcolabÁî®\n",
        "#@title ÂàùÊúüË®≠ÂÆö\n",
        "RUN_SCRIPT = \"petfinder/main.py\" #@param {type:\"string\"}\n",
        "ARGS = \"\" #@param {type:\"string\"}\n",
        "USER_ID = \"near129\" #@param {type:\"string\"}\n",
        "REPO_NAME = \"petfinder\" #@param {type:\"string\"}\n",
        "BRANCH_NAME = 'master' #@param {type:\"string\"}\n",
        "DOWNLOAD_CROP_DATA = True #@param {type:\"boolean\"}\n",
        "import sys\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "\n",
        "def get_seacrets():\n",
        "    if IN_COLAB:\n",
        "        # drive„Åã„ÇâË®≠ÂÆö„Éï„Ç°„Ç§„É´„ÇíË™≠„ÅøËæº„ÇÄ\n",
        "        import json\n",
        "        from pathlib import Path\n",
        "        DRIVE_SETTING_PATH = Path('/content/drive/MyDrive/ml_settings.json')\n",
        "        settings = json.loads(DRIVE_SETTING_PATH.read_text())\n",
        "        return settings['wandb_api'], settings['github_acess_token']\n",
        "    from kaggle_secrets import UserSecretsClient\n",
        "    user_secrets = UserSecretsClient()\n",
        "    return user_secrets.get_secret('wandb_api'), user_secrets.get_secret('github_acess_token')\n",
        "\n",
        "WANDB_API, GITHUB_ACESS_TOKEN = get_seacrets()\n",
        "\n",
        "url = f'https://{USER_ID}:{GITHUB_ACESS_TOKEN}@github.com/{USER_ID}/{REPO_NAME}'\n",
        "!git clone $url -b $BRANCH_NAME\n",
        "%cd $REPO_NAME\n",
        "\n",
        "def get_data():\n",
        "    if IN_COLAB:\n",
        "        !mkdir /root/.kaggle\n",
        "        !cp /content/drive/MyDrive/kaggle.json /root/.kaggle/kaggle.json\n",
        "\n",
        "        %pip install --upgrade --force-reinstall --no-deps kaggle\n",
        "        # !pip install kaggle==1.5.12\n",
        "\n",
        "        !kaggle competitions download -c petfinder-pawpularity-score \n",
        "        !unzip -q petfinder-pawpularity-score.zip -d data\n",
        "\n",
        "        if DOWNLOAD_CROP_DATA:\n",
        "            !kaggle datasets download -d phalanx/petfinder2-cropped-dataset\n",
        "            !unzip -q petfinder2-cropped-dataset.zip\n",
        "            !mv crop data\n",
        "    else:\n",
        "        !cp -r /kaggle/input/petfinder-pawpularity-score data\n",
        "        if DOWNLOAD_CROP_DATA:\n",
        "            !cp -r /kaggle/input/petfinder2-cropped-dataset/crop data\n",
        "\n",
        "get_data()        \n",
        "\n",
        "%pip install -U wandb\n",
        "import wandb\n",
        "wandb.login(key=WANDB_API)\n",
        "\n",
        "import os\n",
        "ARGS += f' general.num_workers={os.cpu_count()}'\n",
        "\n",
        "%pip install poetry\n",
        "!poetry install"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3taoyhUkgxNu",
        "outputId": "ef8a02c0-bf1c-4eb4-b756-7a3e525f7080"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Global seed set to 42\n",
            "/root/.cache/pypoetry/virtualenvs/petfinder-c-wJIZu--py3.7/lib/python3.7/site-packages/sklearn/model_selection/_split.py:680: UserWarning: The least populated class in y has only 4 members, which is less than n_splits=5.\n",
            "  UserWarning,\n",
            "/root/.cache/pypoetry/virtualenvs/petfinder-c-wJIZu--py3.7/lib/python3.7/site-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2157.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
            "[2021-12-10 12:34:21,073][timm.models.helpers][INFO] - Loading pretrained weights from url (https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_tiny_patch4_window7_224.pth)\n",
            "Downloading: \"https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_tiny_patch4_window7_224.pth\" to /root/.cache/torch/hub/checkpoints/swin_tiny_patch4_window7_224.pth\n",
            "GPU available: True, used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnear129\u001b[0m (use `wandb login --relogin` to force relogin)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.7 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.12.6\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mcroped_data_0\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/near129/petfinder\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/near129/petfinder/runs/13qkivm4\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /content/petfinder/outputs/2021-12-10/12-34-09/wandb/run-20211210_123425-13qkivm4\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
            "\n",
            "\n",
            "  | Name      | Type              | Params\n",
            "------------------------------------------------\n",
            "0 | backbone  | SwinTransformer   | 27.5 M\n",
            "1 | fc        | Sequential        | 769   \n",
            "2 | criterion | BCEWithLogitsLoss | 0     \n",
            "------------------------------------------------\n",
            "27.5 M    Trainable params\n",
            "0         Non-trainable params\n",
            "27.5 M    Total params\n",
            "110.080   Total estimated model params size (MB)\n",
            "Global seed set to 42\n",
            "Epoch 0:  91% 140/154 [04:02<00:24,  1.72s/it, loss=0.664, v_num=ivm4]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/31 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 0: 100% 154/154 [04:21<00:00,  1.69s/it, loss=0.664, v_num=ivm4]\n",
            "Epoch 0: 100% 154/154 [04:26<00:00,  1.72s/it, loss=0.66, v_num=ivm4] \n",
            "Epoch 1:  91% 140/154 [04:06<00:24,  1.75s/it, loss=0.659, v_num=ivm4]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 1:  91% 140/154 [04:20<00:25,  1.85s/it, loss=0.659, v_num=ivm4]\n",
            "Epoch 1: 100% 154/154 [04:21<00:00,  1.69s/it, loss=0.659, v_num=ivm4]\n",
            "Epoch 1: 100% 154/154 [04:30<00:00,  1.74s/it, loss=0.658, v_num=ivm4]\n",
            "Epoch 2:  91% 140/154 [04:06<00:24,  1.75s/it, loss=0.65, v_num=ivm4]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/31 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 2: 100% 154/154 [04:25<00:00,  1.71s/it, loss=0.65, v_num=ivm4]\n",
            "Epoch 2: 100% 154/154 [04:29<00:00,  1.74s/it, loss=0.65, v_num=ivm4]\n",
            "Epoch 3:  91% 140/154 [04:05<00:24,  1.74s/it, loss=0.649, v_num=ivm4]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/31 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 3: 100% 154/154 [04:21<00:00,  1.69s/it, loss=0.649, v_num=ivm4]\n",
            "Epoch 3: 100% 154/154 [04:28<00:00,  1.73s/it, loss=0.65, v_num=ivm4] \n",
            "Epoch 4:  91% 140/154 [04:04<00:24,  1.73s/it, loss=0.649, v_num=ivm4]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/31 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 4: 100% 154/154 [04:19<00:00,  1.67s/it, loss=0.649, v_num=ivm4]\n",
            "Epoch 4: 100% 154/154 [04:27<00:00,  1.73s/it, loss=0.65, v_num=ivm4] \n",
            "Epoch 5:  91% 140/154 [04:02<00:24,  1.72s/it, loss=0.646, v_num=ivm4]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/31 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 5: 100% 154/154 [04:21<00:00,  1.69s/it, loss=0.646, v_num=ivm4]\n",
            "Epoch 5: 100% 154/154 [04:25<00:00,  1.71s/it, loss=0.647, v_num=ivm4]\n",
            "Epoch 6:  91% 140/154 [04:03<00:24,  1.73s/it, loss=0.651, v_num=ivm4]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/31 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 6: 100% 154/154 [04:21<00:00,  1.69s/it, loss=0.651, v_num=ivm4]\n",
            "Epoch 6: 100% 154/154 [04:27<00:00,  1.73s/it, loss=0.647, v_num=ivm4]\n",
            "Epoch 7:  91% 140/154 [04:04<00:24,  1.73s/it, loss=0.64, v_num=ivm4]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/31 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 7: 100% 154/154 [04:20<00:00,  1.68s/it, loss=0.64, v_num=ivm4]\n",
            "Epoch 7: 100% 154/154 [04:27<00:00,  1.73s/it, loss=0.64, v_num=ivm4]\n",
            "Epoch 8:  91% 140/154 [04:02<00:24,  1.72s/it, loss=0.646, v_num=ivm4]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/31 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 8: 100% 154/154 [04:22<00:00,  1.69s/it, loss=0.646, v_num=ivm4]\n",
            "Epoch 8: 100% 154/154 [04:25<00:00,  1.71s/it, loss=0.645, v_num=ivm4]\n",
            "Epoch 9:  91% 140/154 [04:02<00:24,  1.72s/it, loss=0.647, v_num=ivm4]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 9:  91% 140/154 [04:16<00:25,  1.82s/it, loss=0.647, v_num=ivm4]\n",
            "Epoch 9: 100% 154/154 [04:17<00:00,  1.66s/it, loss=0.647, v_num=ivm4]\n",
            "Epoch 9: 100% 154/154 [04:25<00:00,  1.71s/it, loss=0.646, v_num=ivm4]\n",
            "Epoch 9: 100% 154/154 [04:25<00:00,  1.72s/it, loss=0.646, v_num=ivm4]\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (/content/petfinder/outputs/2021-12-10/12-34-09)... Done. 0.7s\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 1494... (success).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              lr-AdamW ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÜ‚ñá‚ñà\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train_loss ‚ñÜ‚ñà‚ñÑ‚ñÑ‚ñÉ‚ñá‚ñÖ‚ñÉ‚ñÉ‚ñÜ‚ñÉ‚ñÑ‚ñÖ‚ñÉ‚ñÉ‚ñÑ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÜ‚ñÑ‚ñÇ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train_rmse ‚ñà‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss ‚ñà‚ñÖ‚ñÑ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÖ‚ñÇ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              val_rmse ‚ñà‚ñÖ‚ñÑ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÖ‚ñÇ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 9\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              lr-AdamW 5e-05\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train_loss 0.63437\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train_rmse 17.4004\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 1229\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 0.64585\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              val_rmse 18.34013\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 18 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mcroped_data_0\u001b[0m: \u001b[34mhttps://wandb.ai/near129/petfinder/runs/13qkivm4\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: ./wandb/run-20211210_123425-13qkivm4/logs/debug.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "[2021-12-10 13:19:42,094][timm.models.helpers][INFO] - Loading pretrained weights from url (https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_tiny_patch4_window7_224.pth)\n",
            "GPU available: True, used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.7 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.12.6\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mcroped_data_1\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/near129/petfinder\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/near129/petfinder/runs/3axpbr0y\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /content/petfinder/outputs/2021-12-10/12-34-09/wandb/run-20211210_131942-3axpbr0y\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
            "\n",
            "Validation sanity check: 0it [00:00, ?it/s]\n",
            "  | Name      | Type              | Params\n",
            "------------------------------------------------\n",
            "0 | backbone  | SwinTransformer   | 27.5 M\n",
            "1 | fc        | Sequential        | 769   \n",
            "2 | criterion | BCEWithLogitsLoss | 0     \n",
            "------------------------------------------------\n",
            "27.5 M    Trainable params\n",
            "0         Non-trainable params\n",
            "27.5 M    Total params\n",
            "110.080   Total estimated model params size (MB)\n",
            "Global seed set to 42\n",
            "Epoch 0:  91% 140/154 [04:01<00:24,  1.72s/it, loss=0.662, v_num=br0y]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 0:  91% 140/154 [04:14<00:25,  1.80s/it, loss=0.662, v_num=br0y]\n",
            "Epoch 0: 100% 154/154 [04:16<00:00,  1.66s/it, loss=0.662, v_num=br0y]\n",
            "Epoch 0: 100% 154/154 [04:25<00:00,  1.71s/it, loss=0.663, v_num=br0y]\n",
            "Epoch 1:  91% 140/154 [04:04<00:24,  1.73s/it, loss=0.657, v_num=br0y]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 1:  91% 140/154 [04:14<00:25,  1.81s/it, loss=0.657, v_num=br0y]\n",
            "Epoch 1: 100% 154/154 [04:18<00:00,  1.67s/it, loss=0.657, v_num=br0y]\n",
            "Epoch 1: 100% 154/154 [04:27<00:00,  1.73s/it, loss=0.657, v_num=br0y]\n",
            "Epoch 2:  91% 140/154 [04:03<00:24,  1.73s/it, loss=0.651, v_num=br0y]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/31 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 2: 100% 154/154 [04:23<00:00,  1.70s/it, loss=0.651, v_num=br0y]\n",
            "Epoch 2: 100% 154/154 [04:27<00:00,  1.72s/it, loss=0.651, v_num=br0y]\n",
            "Epoch 3:  91% 140/154 [04:03<00:24,  1.73s/it, loss=0.647, v_num=br0y]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/31 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 3: 100% 154/154 [04:21<00:00,  1.69s/it, loss=0.647, v_num=br0y]\n",
            "Epoch 3: 100% 154/154 [04:27<00:00,  1.72s/it, loss=0.647, v_num=br0y]\n",
            "Epoch 4:  91% 140/154 [04:04<00:24,  1.73s/it, loss=0.65, v_num=br0y]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/31 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 4: 100% 154/154 [04:20<00:00,  1.68s/it, loss=0.65, v_num=br0y]\n",
            "Epoch 4: 100% 154/154 [04:27<00:00,  1.73s/it, loss=0.648, v_num=br0y]\n",
            "Epoch 5:  91% 140/154 [04:03<00:24,  1.73s/it, loss=0.649, v_num=br0y]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 5:  91% 140/154 [04:18<00:25,  1.83s/it, loss=0.649, v_num=br0y]\n",
            "Epoch 5: 100% 154/154 [04:18<00:00,  1.67s/it, loss=0.649, v_num=br0y]\n",
            "Epoch 5: 100% 154/154 [04:27<00:00,  1.72s/it, loss=0.647, v_num=br0y]\n",
            "Epoch 6:  91% 140/154 [04:02<00:24,  1.72s/it, loss=0.646, v_num=br0y]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/31 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 6: 100% 154/154 [04:21<00:00,  1.69s/it, loss=0.646, v_num=br0y]\n",
            "Epoch 6: 100% 154/154 [04:26<00:00,  1.72s/it, loss=0.648, v_num=br0y]\n",
            "Epoch 7:  91% 140/154 [04:06<00:24,  1.75s/it, loss=0.642, v_num=br0y]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 7:  91% 140/154 [04:21<00:25,  1.85s/it, loss=0.642, v_num=br0y]\n",
            "Epoch 7: 100% 154/154 [04:21<00:00,  1.69s/it, loss=0.642, v_num=br0y]\n",
            "Epoch 7: 100% 154/154 [04:30<00:00,  1.74s/it, loss=0.642, v_num=br0y]\n",
            "Epoch 8:  91% 140/154 [04:04<00:24,  1.73s/it, loss=0.648, v_num=br0y]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/31 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 8: 100% 154/154 [04:20<00:00,  1.68s/it, loss=0.648, v_num=br0y]\n",
            "Epoch 8: 100% 154/154 [04:27<00:00,  1.73s/it, loss=0.646, v_num=br0y]\n",
            "Epoch 9:  91% 140/154 [04:04<00:24,  1.73s/it, loss=0.646, v_num=br0y]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/31 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 9: 100% 154/154 [04:22<00:00,  1.70s/it, loss=0.646, v_num=br0y]\n",
            "Epoch 9: 100% 154/154 [04:28<00:00,  1.73s/it, loss=0.646, v_num=br0y]\n",
            "Epoch 9: 100% 154/154 [04:28<00:00,  1.73s/it, loss=0.646, v_num=br0y]\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (/content/petfinder/outputs/2021-12-10/12-34-09)... Done. 1.0s\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 1985... (success).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: W&B sync reduced upload amount by 14.3%             \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              lr-AdamW ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÜ‚ñá‚ñà\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train_loss ‚ñÜ‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñà‚ñÑ‚ñÖ‚ñÖ‚ñÉ‚ñÉ‚ñÖ‚ñÇ‚ñÖ‚ñÉ‚ñÇ‚ñá‚ñÑ‚ñÅ‚ñÇ‚ñÑ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train_rmse ‚ñà‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss ‚ñà‚ñá‚ñÖ‚ñÉ‚ñÉ‚ñá‚ñÅ‚ñÅ‚ñÇ‚ñÉ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              val_rmse ‚ñà‚ñá‚ñÖ‚ñÑ‚ñÉ‚ñÜ‚ñÅ‚ñÅ‚ñÇ‚ñÉ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 9\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              lr-AdamW 5e-05\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train_loss 0.63795\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train_rmse 17.6055\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 1229\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 0.64727\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              val_rmse 18.51094\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 26 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mcroped_data_1\u001b[0m: \u001b[34mhttps://wandb.ai/near129/petfinder/runs/3axpbr0y\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: ./wandb/run-20211210_131942-3axpbr0y/logs/debug.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "[2021-12-10 14:04:56,694][timm.models.helpers][INFO] - Loading pretrained weights from url (https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_tiny_patch4_window7_224.pth)\n",
            "GPU available: True, used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.7 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.12.6\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mcroped_data_2\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/near129/petfinder\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/near129/petfinder/runs/q5psa2jx\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /content/petfinder/outputs/2021-12-10/12-34-09/wandb/run-20211210_140456-q5psa2jx\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
            "\n",
            "\n",
            "  | Name      | Type              | Params\n",
            "------------------------------------------------\n",
            "0 | backbone  | SwinTransformer   | 27.5 M\n",
            "1 | fc        | Sequential        | 769   \n",
            "2 | criterion | BCEWithLogitsLoss | 0     \n",
            "------------------------------------------------\n",
            "27.5 M    Trainable params\n",
            "0         Non-trainable params\n",
            "27.5 M    Total params\n",
            "110.080   Total estimated model params size (MB)\n",
            "Global seed set to 42\n",
            "Epoch 0:  91% 140/154 [04:04<00:24,  1.73s/it, loss=0.658, v_num=a2jx]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/31 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 0: 100% 154/154 [04:20<00:00,  1.68s/it, loss=0.658, v_num=a2jx]\n",
            "Epoch 0: 100% 154/154 [04:27<00:00,  1.73s/it, loss=0.66, v_num=a2jx] \n",
            "Epoch 1:  91% 140/154 [04:06<00:24,  1.75s/it, loss=0.655, v_num=a2jx]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 1:  91% 140/154 [04:18<00:25,  1.83s/it, loss=0.655, v_num=a2jx]\n",
            "Epoch 1: 100% 154/154 [04:21<00:00,  1.69s/it, loss=0.655, v_num=a2jx]\n",
            "Epoch 1: 100% 154/154 [04:30<00:00,  1.74s/it, loss=0.656, v_num=a2jx]\n",
            "Epoch 2:  91% 140/154 [04:06<00:24,  1.75s/it, loss=0.653, v_num=a2jx]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/31 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 2: 100% 154/154 [04:24<00:00,  1.71s/it, loss=0.653, v_num=a2jx]\n",
            "Epoch 2: 100% 154/154 [04:30<00:00,  1.74s/it, loss=0.654, v_num=a2jx]\n",
            "Epoch 3:  91% 140/154 [04:06<00:24,  1.75s/it, loss=0.647, v_num=a2jx]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 3:  91% 140/154 [04:20<00:25,  1.85s/it, loss=0.647, v_num=a2jx]\n",
            "Epoch 3: 100% 154/154 [04:21<00:00,  1.69s/it, loss=0.647, v_num=a2jx]\n",
            "Epoch 3: 100% 154/154 [04:29<00:00,  1.74s/it, loss=0.647, v_num=a2jx]\n",
            "Epoch 4:  91% 140/154 [04:05<00:24,  1.74s/it, loss=0.643, v_num=a2jx]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 4:  91% 140/154 [04:16<00:25,  1.82s/it, loss=0.643, v_num=a2jx]\n",
            "Epoch 4: 100% 154/154 [04:20<00:00,  1.68s/it, loss=0.643, v_num=a2jx]\n",
            "Epoch 4: 100% 154/154 [04:29<00:00,  1.74s/it, loss=0.644, v_num=a2jx]\n",
            "Epoch 5:  91% 140/154 [04:06<00:24,  1.75s/it, loss=0.647, v_num=a2jx]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/31 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 5: 100% 154/154 [04:22<00:00,  1.70s/it, loss=0.647, v_num=a2jx]\n",
            "Epoch 5: 100% 154/154 [04:29<00:00,  1.74s/it, loss=0.648, v_num=a2jx]\n",
            "Epoch 6:  91% 140/154 [04:05<00:24,  1.74s/it, loss=0.646, v_num=a2jx]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 6:  91% 140/154 [04:18<00:25,  1.84s/it, loss=0.646, v_num=a2jx]\n",
            "Epoch 6: 100% 154/154 [04:20<00:00,  1.68s/it, loss=0.646, v_num=a2jx]\n",
            "Epoch 6: 100% 154/154 [04:29<00:00,  1.74s/it, loss=0.645, v_num=a2jx]\n",
            "Epoch 7:  91% 140/154 [04:05<00:24,  1.74s/it, loss=0.642, v_num=a2jx]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 7:  91% 140/154 [04:15<00:25,  1.81s/it, loss=0.642, v_num=a2jx]\n",
            "Epoch 7: 100% 154/154 [04:20<00:00,  1.68s/it, loss=0.642, v_num=a2jx]\n",
            "Epoch 7: 100% 154/154 [04:29<00:00,  1.74s/it, loss=0.645, v_num=a2jx]\n",
            "Epoch 8:  91% 140/154 [04:01<00:24,  1.72s/it, loss=0.643, v_num=a2jx]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 8:  91% 140/154 [04:16<00:25,  1.82s/it, loss=0.643, v_num=a2jx]\n",
            "Epoch 8: 100% 154/154 [04:16<00:00,  1.66s/it, loss=0.643, v_num=a2jx]\n",
            "Epoch 8: 100% 154/154 [04:25<00:00,  1.71s/it, loss=0.642, v_num=a2jx]\n",
            "Epoch 9:  91% 140/154 [04:03<00:24,  1.73s/it, loss=0.634, v_num=a2jx]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 9:  91% 140/154 [04:17<00:25,  1.82s/it, loss=0.634, v_num=a2jx]\n",
            "Epoch 9: 100% 154/154 [04:18<00:00,  1.67s/it, loss=0.634, v_num=a2jx]\n",
            "Epoch 9: 100% 154/154 [04:27<00:00,  1.73s/it, loss=0.635, v_num=a2jx]\n",
            "Epoch 10:  91% 140/154 [04:02<00:24,  1.72s/it, loss=0.641, v_num=a2jx]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/31 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 10: 100% 154/154 [04:19<00:00,  1.68s/it, loss=0.641, v_num=a2jx]\n",
            "Epoch 10: 100% 154/154 [04:26<00:00,  1.72s/it, loss=0.638, v_num=a2jx]\n",
            "Epoch 11:  91% 140/154 [04:04<00:24,  1.73s/it, loss=0.636, v_num=a2jx]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/31 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 11: 100% 154/154 [04:19<00:00,  1.67s/it, loss=0.636, v_num=a2jx]\n",
            "Epoch 11: 100% 154/154 [04:27<00:00,  1.73s/it, loss=0.638, v_num=a2jx]\n",
            "Epoch 12:  91% 140/154 [04:02<00:24,  1.72s/it, loss=0.636, v_num=a2jx]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/31 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 12: 100% 154/154 [04:21<00:00,  1.69s/it, loss=0.636, v_num=a2jx]\n",
            "Epoch 12: 100% 154/154 [04:26<00:00,  1.72s/it, loss=0.637, v_num=a2jx]\n",
            "Epoch 13:  91% 140/154 [04:02<00:24,  1.72s/it, loss=0.628, v_num=a2jx]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 13:  91% 140/154 [04:15<00:25,  1.81s/it, loss=0.628, v_num=a2jx]\n",
            "Epoch 13: 100% 154/154 [04:17<00:00,  1.66s/it, loss=0.628, v_num=a2jx]\n",
            "Epoch 13: 100% 154/154 [04:26<00:00,  1.72s/it, loss=0.63, v_num=a2jx] \n",
            "Epoch 13: 100% 154/154 [04:26<00:00,  1.72s/it, loss=0.63, v_num=a2jx]\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (/content/petfinder/outputs/2021-12-10/12-34-09)... Done. 1.7s\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 2294... (success).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: W&B sync reduced upload amount by 18.2%             \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              lr-AdamW ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñá‚ñà\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train_loss ‚ñá‚ñá‚ñà‚ñÜ‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñÉ‚ñÑ‚ñá‚ñÑ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÜ‚ñÑ‚ñá‚ñÖ‚ñÖ‚ñÑ‚ñÜ‚ñÉ‚ñÜ‚ñÑ‚ñÉ‚ñÇ‚ñÜ‚ñÇ‚ñÉ‚ñÖ‚ñÑ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train_rmse ‚ñà‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss ‚ñà‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÜ‚ñÇ‚ñÑ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              val_rmse ‚ñà‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÖ‚ñÇ‚ñÑ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 13\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              lr-AdamW 8e-05\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train_loss 0.6127\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train_rmse 15.74797\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 1721\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 0.64857\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              val_rmse 18.70272\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 37 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mcroped_data_2\u001b[0m: \u001b[34mhttps://wandb.ai/near129/petfinder/runs/q5psa2jx\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: ./wandb/run-20211210_140456-q5psa2jx/logs/debug.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "[2021-12-10 15:08:23,943][timm.models.helpers][INFO] - Loading pretrained weights from url (https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_tiny_patch4_window7_224.pth)\n",
            "GPU available: True, used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.7 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.12.6\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mcroped_data_3\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/near129/petfinder\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/near129/petfinder/runs/3fdnt35y\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /content/petfinder/outputs/2021-12-10/12-34-09/wandb/run-20211210_150824-3fdnt35y\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
            "\n",
            "\n",
            "  | Name      | Type              | Params\n",
            "------------------------------------------------\n",
            "0 | backbone  | SwinTransformer   | 27.5 M\n",
            "1 | fc        | Sequential        | 769   \n",
            "2 | criterion | BCEWithLogitsLoss | 0     \n",
            "------------------------------------------------\n",
            "27.5 M    Trainable params\n",
            "0         Non-trainable params\n",
            "27.5 M    Total params\n",
            "110.080   Total estimated model params size (MB)\n",
            "Global seed set to 42\n",
            "Epoch 0:  13% 20/154 [00:41<04:25,  1.98s/it, loss=0.681, v_num=t35y]/usr/lib/python3.7/multiprocessing/semaphore_tracker.py:144: UserWarning: semaphore_tracker: There appear to be 24 leaked semaphores to clean up at shutdown\n",
            "  len(cache))\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "!poetry run python $RUN_SCRIPT $ARGS"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyPeECvTxyOijPcuLkkBIwKK",
      "collapsed_sections": [],
      "include_colab_link": true,
      "mount_file_id": "1npE3nehGQTzEzj-lnjyeFMjCNjcwfXeW",
      "name": "run_exp.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
